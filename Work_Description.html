<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>MTP:</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(249, 228, 188, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="87086821-9b7b-4b64-aef9-516496dcef10" class="page sans"><header><div class="page-header-icon undefined"><img class="icon" src="https://www.notion.so/icons/book_brown.svg"/></div><h1 class="page-title">MTP:</h1><p class="page-description"></p></header><div class="page-body"><h2 id="dca105b2-786e-4ae7-bb46-d895d9c91f1b" class=""><mark class="highlight-orange">Title:-Evaluation Of LLM based models for Relation-Extraction or Relation-Classification</mark></h2><p id="f68d77be-6f94-464b-a49b-8f63baf2a316" class=""><strong>AmalREC</strong> is a dataset designed to improve relation extraction (RE) and relation classification (RC) tasks in natural language processing (NLP). It addresses limitations in existing datasets, such as limited relation types and domain-specific biases, by providing a more diverse and complex set of relation types.</p><ul id="621a181a-3e7f-41ed-b21f-629c9bf15437" class="toggle"><li><details open=""><summary><mark class="highlight-brown">Methodology:</mark></summary><p id="42ed36bf-a29d-418e-9f64-91197b08857b" class="">The project is divided into five stages:</p><ol type="1" id="84d3e36b-8c39-4b97-80df-9327438f905f" class="numbered-list" start="1"><li><strong>Tuple Collection:</strong><ul id="ec524274-f8fc-4697-a9a9-600e57cf75b2" class="bulleted-list"><li style="list-style-type:disc">Collect relation tuples from a dataset with 255 distinct relation buckets extracted from DBpedia.</li></ul><ul id="82fa5d9e-2db0-43ed-a5d1-f6dd407784bd" class="bulleted-list"><li style="list-style-type:disc">Select a balanced subset of tuples, ensuring approximately 195,000 instances.</li></ul></li></ol><ol type="1" id="c542d263-656f-4633-9dcc-848137193e5a" class="numbered-list" start="2"><li><strong>Data Generation:</strong><ul id="bf8b8dc5-3a90-4bab-b8eb-b39df5e10e9a" class="bulleted-list"><li style="list-style-type:disc">Implement 15 different methods across five paradigms:<ul id="73657223-8fe1-40fe-a6b9-d6db80827741" class="bulleted-list"><li style="list-style-type:circle">Template-based generation using human-generated templates.</li></ul><ul id="7b2bdfe0-e3ed-427b-9b2a-5c93fdc7bc6d" class="bulleted-list"><li style="list-style-type:circle">Encoder-decoder models like T5 and BART.</li></ul><ul id="7217ac78-bd63-465f-8b7c-0ff736f0748c" class="bulleted-list"><li style="list-style-type:circle">Decoder-only models such as GPT 3.5, LLaMa, and PaLM.</li></ul><ul id="7b5a7d46-4788-4705-8d40-300b7e02453f" class="bulleted-list"><li style="list-style-type:circle">Fusion techniques combine the strengths of encoder-decoder and decoder-only models.</li></ul><ul id="f1b05ddf-8a83-4017-b869-c9438d4d28df" class="bulleted-list"><li style="list-style-type:circle">Extended context-based generation using external data from Wikipedia.</li></ul></li></ul></li></ol><p id="2783914c-60d8-48bf-841b-99d18df94cea" class="">
</p><ol type="1" id="2db15545-3576-4e4b-af93-a1e24b277b9d" class="numbered-list" start="3"><li><strong>Evaluation : </strong><br/>To evaluate the model or generated sentences we can use quality parameters like BLEU, METEOR, grammar, sentiment, and coherence to evaluate generated sentences.<br/>Rank sentences based on these parameters to select the best ones.<br/></li></ol><p id="2d9bb2e7-58d0-43c4-aed3-f5424e660eab" class=""><br/>4. <br/><strong>Impact:</strong><div class="indented"><p id="0bf113d7-ae64-4c8b-a4fa-bfe426b3d22d" class="">By addressing existing RE/RC datasets limitations, AmalREC aims to enhance the effectiveness of NLP tasks like information extraction, knowledge base construction, and question-answering. The dataset&#x27;s diversity and quality make it a valuable resource for training more robust models.</p></div></p><p id="d67de69c-c50f-43f2-a764-5b9935ec7f2a" class="">
</p></details></li></ul><ul id="50a7ae69-8909-4d14-bf1d-088fef2b02b4" class="toggle"><li><details open=""><summary><mark class="highlight-brown">Evaluation Metric??</mark></summary><p id="d675f686-92df-40c4-9eab-9342a03dfbc7" class="">Qualitative and Quantitative parameters for existing relation extraction datasets:</p><ol type="1" id="d568f2a6-a6ff-44ad-884b-a81fddfcc815" class="numbered-list" start="1"><li>Qualitative parameters:<br/>Characteristics like diversity, complexity, domain coverage, and language variety within datasets.<br/>Assess the dataset&#x27;s usability and applicability in real-world scenarios.<br/></li></ol><ol type="1" id="805795d8-553a-4af3-8449-1051ca8b051d" class="numbered-list" start="2"><li>Quantitative parameters:<br/>Statistical measures such as the number of relation types, data size, distribution of relation instances, etc.<br/>Performance metrics like precision, recall, and F1 score on benchmark tasks.<br/></li></ol><p id="e9465a0d-603e-49a1-ba0b-87cf44eaecc3" class=""><br/><br/><strong>Previously used metrics:</strong></p><ol type="1" id="e0f73c93-15e2-4359-91cd-5ec860a8ee6f" class="numbered-list" start="1"><li><mark class="highlight-brown"><strong>MoverScore: </strong></mark><mark class="highlight-brown"><br/><br/></mark><br/>MoverScore measures the semantic similarity between reference and candidate sentences by calculating the distances between word embeddings (vector representations of words).<br/><br/></li></ol><p id="a6e0a550-b953-40cb-a2ab-a421a18685ec" class=""><strong>How It Works:</strong></p><ul id="5cca6799-9b93-4341-ac89-ea5fe540545f" class="bulleted-list"><li style="list-style-type:disc">It uses word embeddings to represent the meaning of words in high-dimensional space.</li></ul><ul id="0e9ed981-128e-43c3-a29a-1a9b90ce4a1c" class="bulleted-list"><li style="list-style-type:disc">Calculate the distance between the embeddings of words in the reference sentence and those in the candidate sentence.</li></ul><ul id="2e829e4a-f657-48d0-b66b-4aca44845d69" class="bulleted-list"><li style="list-style-type:disc">Aggregates these distances to provide a similarity score that reflects how closely the candidate sentence matches the reference in terms of meaning.</li></ul><p id="7585ceaf-cdc8-48db-ac87-76628e54deab" class="">It is useful in scenarios where semantic similarity is important.</p><p id="82677dbd-6fb1-4d4a-ba77-98bdddd7669d" class=""><strong>Some benefits of using it as a metric are:</strong></p><ul id="11457b1c-df38-49c2-bb15-e393de95b3f8" class="bulleted-list"><li style="list-style-type:disc">Captures semantic similarity better than surface-level metrics like BLEU, which rely on exact word matches.</li></ul><ul id="d8e6c83c-8064-4537-8005-6d9725397e20" class="bulleted-list"><li style="list-style-type:disc">Can handle synonyms and paraphrases effectively.</li></ul><p id="f2f55a4f-dd37-46b6-be5b-d8c65b394866" class=""><strong>Limitations:</strong></p><ul id="240c3865-d76f-4244-b9be-a925252c6863" class="bulleted-list"><li style="list-style-type:disc">Depending on the quality of word embeddings used, poor embeddings can lead to inaccurate scores.</li></ul><ul id="ab3ea2bb-b6f0-494d-8818-db2fb0c30e35" class="bulleted-list"><li style="list-style-type:disc">May not fully capture sentence structure and syntax.<br/><br/></li></ul><ol type="1" id="80604c82-92c6-4688-8e8b-8ffaeeb86f46" class="numbered-list" start="2"><li><mark class="highlight-brown"><strong>Parent:</strong></mark><mark class="highlight-brown"><br/><br/></mark><mark class="highlight-default">This is a metric that is specifically designed for evaluating data for text-generation tasks. considering both precision and recall with a focus on semantic content.<br/><br/></mark><p id="69ef7a6c-beca-4a96-a55a-8f4ef3c3cfde" class=""><strong>How It Works:</strong></p><ul id="f2f7aa34-88fc-4e4a-b011-945a33d6d951" class="bulleted-list"><li style="list-style-type:disc">Compares the candidate sentence to both the reference sentence and the input data used to generate the text.</li></ul><ul id="3085dbdc-0d86-494d-9d9d-b40b8ff8e44c" class="bulleted-list"><li style="list-style-type:disc">Measures precision (how much of the candidate content is relevant) and recall (how much of the reference content is covered) while ensuring semantic alignment.</li></ul><p id="edcd1514-352b-4b57-835f-3597249459ea" class="">
</p><p id="ddc7c276-9ee5-4d9f-afda-9525c458357d" class=""><strong>The benefits of using this metric are:</strong></p><ul id="0dc5ac4d-4e53-4677-ad78-622faf39cd4d" class="bulleted-list"><li style="list-style-type:disc">Considers the input data, making it well-suited for data-to-text tasks where the context is essential.</li></ul><ul id="e66bbaee-101b-41eb-bafa-4d3b62f7a8d3" class="bulleted-list"><li style="list-style-type:disc">Balances precision and recall to provide a comprehensive evaluation.</li></ul><p id="0d36ff22-87bb-4f57-8282-73f55d3b9be3" class=""><br/><br/><strong>Limitations:</strong></p><ul id="c746421d-7c10-42ac-bf9a-022cd1a811eb" class="bulleted-list"><li style="list-style-type:disc">Requires access to both reference texts and input data, which may not always be available.</li></ul><ul id="9faddf97-b32e-460b-9846-bea050feafaa" class="bulleted-list"><li style="list-style-type:disc">May be computationally intensive due to the dual comparison process.</li></ul><p id="688188f3-d200-4602-8b2c-d20dc66a8e4a" class="">
</p></li></ol><ol type="1" id="24f73534-b0a3-4ae8-a684-be05da603164" class="numbered-list" start="3"><li><mark class="highlight-brown"><strong>BARTScore:<br/><br/><br/></strong></mark><mark class="highlight-default">BARTScore uses BART a denoising autoencoder to evaluate the sentence based on their likelihood that asses both fluency and semantic correctness.<br/><br/></mark><p id="2cde1e81-c122-49b7-adc5-d688533dd783" class=""><strong>How It Works:</strong></p><ul id="ae7ffa3a-6be6-4423-a745-36e4b3a0aeb4" class="bulleted-list"><li style="list-style-type:disc">Calculates the likelihood of the candidate sentence being generated by BART given the reference sentence.</li></ul><ul id="0d921255-86f4-479c-b82f-219da6bd5935" class="bulleted-list"><li style="list-style-type:disc">Higher scores indicate that the candidate is more fluent and semantically similar to the reference.</li></ul><p id="556e1f3c-0025-4db6-8258-7b619f5dc4a0" class=""><br/>More useful for evaluating the quality of text generation tasks where fluency and coherence are critical, such as summarization and dialogue generation.<br/></p><p id="9e273df6-bb77-4e57-b20c-4c901fd8dbe4" class="">
</p><p id="06405a70-b11d-4b6d-a787-ad4c5f4f60ce" class=""><strong>Benefits:</strong></p><ul id="a576fc6d-fd05-45cb-8dc9-414f56537b98" class="bulleted-list"><li style="list-style-type:disc">Takes into account both fluency and semantic accuracy, providing a balanced evaluation.</li></ul><ul id="08b726da-3b17-4591-92e2-be6fbfc81536" class="bulleted-list"><li style="list-style-type:disc">Utilizes the powerful capabilities of BART for natural language understanding and generation.</li></ul><p id="65f31931-e243-40e4-859c-8d631d29ba7c" class="">
</p><p id="388fc1c4-a10e-4e13-855e-d5d896d86c41" class=""><strong>Limitations:</strong></p><ul id="ae0761ba-eb0a-4678-b034-f8f61e65f03e" class="bulleted-list"><li style="list-style-type:disc">Requires fine-tuning BART on the specific task, which can be resource-intensive.</li></ul><ul id="5425d639-bb4a-4977-b42f-5f11dc66303d" class="bulleted-list"><li style="list-style-type:disc">The metric&#x27;s effectiveness depends on BART&#x27;s ability to model the specific domain or language style.<br/><br/></li></ul></li></ol><ol type="1" id="013c09fc-352a-4089-bca3-5cc50e341fc6" class="numbered-list" start="4"><li><mark class="highlight-brown"><strong>BERTScore:<br/><br/><br/></strong></mark>It uses the concept like MoverScore but a little different MoverScore focuses on sentence-level semantic movement, capturing broader shifts in meaning across sentences. While BertScore focuses on token-level alignment and similarity using BERT’s contextual embeddings.<br/><br/><p id="e9da1844-f1ec-46c2-ae1a-3b164c9d9944" class=""><strong>How It Works:</strong></p><ul id="10314937-ab0c-4bbd-b595-88fc382015d3" class="bulleted-list"><li style="list-style-type:disc"><strong>Token-Level Embeddings:</strong> BERTScore computes similarity by comparing embeddings for each token (word) in the candidate sentence with those in the reference sentence, using contextual embeddings from the BERT model.</li></ul><ul id="dbf457d5-c02a-40a1-9cb9-699755ecf9cb" class="bulleted-list"><li style="list-style-type:disc"><strong>Contextual Understanding:</strong> It leverages BERT&#x27;s ability to understand context, capturing nuances in meaning and variations in sentence structure.</li></ul><ul id="c2e2239c-53b1-477b-b18f-765f22c71d74" class="bulleted-list"><li style="list-style-type:disc"><strong>Similarity Computation:</strong> Typically calculates precision, recall, and F1-score based on the maximum similarity for each token in the candidate to any token in the reference, and vice versa.</li></ul><p id="1b322a07-47f3-4439-a2c4-e3b84ab3df93" class=""><div class="indented"><p id="d75be63d-bf2a-468c-9b9a-2b0bdc2d5bcf" class=""><strong>Benefits:</strong></p><ul id="c07ee655-3e66-4767-804a-5976e5276a1d" class="bulleted-list"><li style="list-style-type:disc">Highly effective at capturing semantic nuances due to BERT&#x27;s contextual embeddings.</li></ul><ul id="ee0085d5-4aa2-4979-a0cf-81d1f8e42d68" class="bulleted-list"><li style="list-style-type:disc">More robust to word order variations and synonyms compared to traditional metrics.</li></ul><p id="88c992bc-242e-4208-a371-550f0f89b69e" class="">
</p><p id="532ad782-4b2e-4994-9eb5-05010f97da0f" class=""><strong>Limitations:</strong></p><ul id="592ac3e9-8514-4a71-bfaa-b7b2cf7d8266" class="bulleted-list"><li style="list-style-type:disc">Can be computationally expensive, especially for long texts.</li></ul><ul id="6075c0f3-0507-495e-b127-8245c3facb95" class="bulleted-list"><li style="list-style-type:disc">The metric&#x27;s accuracy relies on the pre-trained BERT model&#x27;s understanding of the language and domain.</li></ul><p id="c1a144c4-be4b-4225-8aa4-bfc5ef6395e7" class="">
</p></div></p></li></ol><ol type="1" id="4f80c5f7-ff4f-421d-8579-1ef122b9e1d9" class="numbered-list" start="5"><li><mark class="highlight-brown"><strong>FactScore:</strong></mark></li></ol><p id="6105d0d6-6366-4394-9e9e-19287759da59" class="">FACTScore is a metric used to assess whether the content generated by a model is factually accurate compared to a reference source. It focuses on verifying that the information provided in the generated text aligns with factual data or known truths.</p><p id="4462c854-296d-4684-9a7d-957f4a577c5a" class="">
</p><p id="a483786c-f105-4e9e-8aa2-1f2c6ad4c6b1" class=""><strong>How it works:</strong></p><ul id="fee755f2-c352-42fe-ad49-8def91a8d72e" class="bulleted-list"><li style="list-style-type:disc">FACTScore compares the generated text (hypothesis) to a reference text that is assumed to be factually accurate.</li></ul><ul id="0d374dbf-e931-43a6-b065-325c7c9e81ff" class="bulleted-list"><li style="list-style-type:disc">It checks for alignment in terms of entities, relationships, and factual statements.</li></ul><ul id="715c7a30-8be8-48d5-92d0-50dfcdf53382" class="bulleted-list"><li style="list-style-type:disc">The metric outputs a score indicating the level of factual consistency. A higher score implies greater alignment with the truth.</li></ul></details></li></ul><ul id="1371a379-fb8c-45b8-ab23-ede871a45851" class="toggle"><li><details open=""><summary><mark class="highlight-brown"><strong>NOTES:</strong></mark></summary><ol type="1" id="acba09f7-697c-4ed8-9d9b-d428a2bbe06a" class="numbered-list" start="1"><li>We have seen that metrics that rely solely on reference texts, such as <strong>BLEU </strong>and <strong>ROUGE</strong>, show poor correlation with human judgments when those references diverge.</li></ol><ol type="1" id="fc4ecc80-9d61-481d-a6c5-8f665b2ecc44" class="numbered-list" start="2"><li>For table-to-text generation, automatic evaluation has largely relied on BLEU and ROUGE<br/>The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information that no system can be expected to produce given only the associated table. We call such reference texts divergent from the table.<br/></li></ol><ol type="1" id="d8e1f92b-da13-4279-a19b-4ca1582fad28" class="numbered-list" start="3"><li>Why This Matters<ol type="a" id="fbf02288-bbcc-4514-8488-9478a35d728e" class="numbered-list" start="1"><li><strong>Divergent References</strong>: In many datasets, the reference texts used for evaluation include information not present in the associated tables, leading to discrepancies. This divergence causes traditional metrics, which assume the reference is a gold standard, to misrepresent the true quality of the generated text.</li></ol><ol type="a" id="fec575f9-adc6-4553-ab4e-1b5f9979ac2f" class="numbered-list" start="2"><li><strong>Human Evaluation</strong>: While human judgment is the most reliable way to evaluate text quality, it is costly and impractical for large-scale or frequent assessments.</li></ol><ol type="a" id="1ece9aad-92ef-4620-92a8-319f3df9fca3" class="numbered-list" start="3"><li><strong>Improved Evaluation Metrics</strong>: Developing automatic metrics that better correlate with human judgment, especially in cases of divergence, is crucial for advancing the field of natural language generation.</li></ol><p id="ddbc81d4-c08c-44a2-ad44-4f1ec409a1bc" class="">
</p></li></ol><p id="135407c9-12b5-4d89-bcd3-e9126d9073e8" class="">A new metric called <strong>PARENT</strong> (Precision And Recall of Entailed N-grams from the Table). The main features and benefits of PARENT are:</p><ul id="5251a157-40e8-486b-8a64-b92a394457af" class="bulleted-list"><li style="list-style-type:disc"><strong>Combines Reference and Table Data</strong>: PARENT evaluates generated text by comparing it to both the reference text and the original table data. This approach helps in recognizing and rewarding content that is correct and relevant, even if it&#x27;s not present in the reference text.</li></ul><ul id="c8a016f3-601c-4f51-8f52-476e940e627d" class="bulleted-list"><li style="list-style-type:disc"><strong>Handles Divergence</strong>: By considering the table as part of the evaluation process, PARENT effectively manages cases where the reference diverges from the table. It rewards generated text that accurately reflects the table content, even if it diverges from the reference.</li></ul><ul id="d0d2f222-8ad9-455b-b231-b834fae4b169" class="bulleted-list"><li style="list-style-type:disc"><strong>Correlation with Human Judgments</strong>: The metric is designed to better align with human judgments by penalizing incorrect information and rewarding the inclusion of correct data from the tables.</li></ul></details></li></ul><ul id="f30b9ff2-1c39-49b4-84ee-43bc46baa3d3" class="toggle"><li><details open=""><summary><mark class="highlight-brown"><strong>current work using BERTScore:</strong></mark></summary><ol type="1" id="d5d29531-f579-4cf8-b232-0d6f403390e1" class="numbered-list" start="1"><li>when candidate sentence=silver sentence and reference =amalREC test sentence</li></ol><p id="4b679b33-b6f9-4475-9b6d-2c22dbc2bfdb" class="">metrics outputs { precision, recall, f1-score} are as follows</p><p id="34ab79e9-53d8-487c-9e8a-827477cf73f9" class="">Precision: 0.3371<br/>Recall: 0.5033<br/>F1 Score: 0.4191<br/></p><p id="e05b0785-7fbb-4e9f-91ea-e6ffd67d603c" class=""><mark class="highlight-default">when candidate sentence=gold sentence and reference =amalREC test sentence</mark></p><p id="627e1703-08a7-4ca7-9565-01b7753633df" class=""><mark class="highlight-default">metric’s outputs { precision, recall, f1-score} are as follows</mark></p><p id="36b63321-dd2a-4b9b-b2f8-bd040d4ad410" class=""><mark class="highlight-default">Precision: 0.3803<br/>Recall: 0.5489<br/>F1 Score: 0.4635<br/></mark></p><p id="4f6ca697-6638-4957-9eee-85da98e1fb82" class=""><mark class="highlight-default">Hence we will take gold sentence and amalREC test sentence for evaluation metric.</mark></p><p id="94913c14-0f19-4cce-8de3-c5b9a15862fe" class="">
</p></details></li></ul><ul id="4992b552-0d66-40ec-86e3-a57f58fab47e" class="toggle"><li><details open=""><summary><mark class="highlight-brown"><strong>Work with MoverScore:</strong></mark></summary><p id="f5e6767a-ca89-4777-bcd0-c6f29b2f185c" class="">MoverScore measures the semantic similarity between reference and candidate sentences by calculating the distances between word embeddings (vector representations of words).</p><p id="4db1d255-7d4f-469a-b721-79c4a60cbd41" class=""><strong>How It Works:</strong></p><ul id="8e2fa2b7-987c-4e30-bd23-5c4fd62f179e" class="bulleted-list"><li style="list-style-type:disc">It uses word embeddings to represent the meaning of words in high-dimensional space.</li></ul><ul id="893d204c-0724-451d-8217-1e1a71851aed" class="bulleted-list"><li style="list-style-type:disc">Calculate the distance between the embeddings of words in the reference sentence and those in the candidate sentence.</li></ul><ul id="12e01122-d8ef-403e-ad28-4ea14c06fa38" class="bulleted-list"><li style="list-style-type:disc">Aggregates these distances to provide a similarity score that reflects how closely the candidate sentence matches the reference in terms of meaning.</li></ul><ul id="88ad9e09-48da-4428-b69a-9b85c0914e3b" class="toggle"><li><details open=""><summary><mark class="highlight-brown"><strong>MoverScore Calculation Process:</strong></mark></summary><ol type="1" id="4527195f-4b18-492f-8744-79c5a59a8bec" class="numbered-list" start="1"><li><strong>Word Mover’s Distance (WMD)</strong>:<ul id="a9a058c0-e4c5-418e-84e8-c0e52ceb5fd2" class="bulleted-list"><li style="list-style-type:disc">The key concept behind MoverScore is the Word Mover&#x27;s Distance (WMD). WMD calculates the &quot;cost&quot; of transforming the word embeddings of the predicted sentence into those of the reference sentence. This cost is the sum of distances between matched words, minimized over all possible word matches.</li></ul></li></ol><ol type="1" id="03df2893-2cde-45e6-9a72-bd23e2292c73" class="numbered-list" start="2"><li><strong>Inverse Relationship</strong>:<ul id="543e45e2-4c62-4705-8d3d-f84b913f0566" class="bulleted-list"><li style="list-style-type:disc">A lower WMD indicates that the predicted sentence is semantically close to the reference because less &quot;movement&quot; is required to match their word embeddings.</li></ul></li></ol><ol type="1" id="754aff88-016d-4353-98cd-80c7b06935e5" class="numbered-list" start="3"><li><strong>Normalization and Inversion</strong>:<ul id="95977981-32f0-4b1f-8f51-796a4a6cc1d4" class="bulleted-list"><li style="list-style-type:disc">MoverScore inverts and normalizes this distance. Instead of directly using the WMD as the score, MoverScore takes the inverse of the distance to ensure that semantically closer sentences (lower WMD) result in a higher MoverScore.</li></ul></li></ol></details></li></ul><ul id="06f04aef-364f-4741-bdb5-d584c9d73b11" class="toggle"><li><details open=""><summary><mark class="highlight-brown">WMD(Word Mover’s Distance) calculation process:</mark></summary><p id="3e28c7d0-77d2-4239-a985-f9daa4249328" class=""><strong>Word Mover&#x27;s Distance (WMD)</strong> is a measure of the distance between two texts based on word embeddings (like those from BERT). Here&#x27;s how it works:</p><ol type="1" id="538d9c96-5390-4baa-a483-7d27407f3e05" class="numbered-list" start="1"><li><strong>Word Embeddings</strong>: Each word in the text is converted into a high-dimensional vector using a pre-trained model like BERT. These vectors capture the meaning of the words in context.</li></ol><ol type="1" id="12097e42-fa99-43a9-80f9-e7d321bd2a29" class="numbered-list" start="2"><li><strong>Transportation Problem</strong>: WMD treats one text as a distribution of these word embeddings. It calculates how much &quot;effort&quot; it takes to move the word embeddings of the generated text to match the word embeddings of the reference text.</li></ol><ol type="1" id="1da60556-5488-4d11-8129-ce9cb877fe09" class="numbered-list" start="3"><li><strong>Optimal Matching</strong>: The goal is to find the most efficient way (with the least total &quot;movement&quot;) to align the words from the generated text with the reference text, considering the semantic similarity of the words.</li></ol><ol type="1" id="20287c13-a951-43d0-b910-f4d6280d1831" class="numbered-list" start="4"><li><strong>Distance Calculation</strong>: The total cost of this &quot;movement&quot; gives us the Word Mover&#x27;s Distance. A lower WMD means the texts are more similar.</li></ol></details></li></ul><p id="45d86a48-0fc8-42b3-843d-df692f3a1b50" class=""><strong>Output:</strong></p><p id="97a1b650-ac31-4c2a-a4d0-b14b3d8287da" class="">MoverScore: 0.2481816465633771 (for amalREC dataset)</p><p id="74a9b337-1d8a-4211-a1db-e13b8835b9de" class="">Some Keypoints:</p><ol type="1" id="46aba5bf-006a-4c4d-830d-28e1cdae5ef4" class="numbered-list" start="1"><li>MoverScore typically uses BERT (Bidirectional Encoder Representations from Transformers) to generate word embeddings for calculating the Word Mover&#x27;s Distance. Specifically, the pre-trained BERT model provides contextualized embeddings for each word in the text.<br/><br/><br/></li></ol><ol type="1" id="dbb1f3b4-8c84-42db-8384-7bd94ec26fca" class="numbered-list" start="2"><li>BERT, as a pre-trained model, does have embeddings for a vast number of words, but not for every possible word. Here’s what happens in cases where a word is not directly in BERT’s vocabulary: <br/><br/><strong>Subword Tokenization</strong>: BERT uses a technique called WordPiece tokenization. If a word is not in its vocabulary, it is broken down into smaller subword units or tokens, each of which has an embedding.<br/><br/><strong>Embedding for Unknown Words</strong>: The embeddings for these subword tokens are then combined to form the overall representation of the original word.<br/><br/><br/></li></ol><ol type="1" id="9008f442-bfc7-4828-bac5-c044bdd80c19" class="numbered-list" start="3"><li>the distance between word embeddings in the Word Mover&#x27;s Distance (WMD) calculation is typically based on <strong>cosine similarity</strong>.</li></ol><ul id="d75c9b22-1b8f-4738-a1df-532692dd4df2" class="bulleted-list"><li style="list-style-type:disc"><strong>Cosine Similarity</strong>: It measures the cosine of the angle between two vectors (word embeddings), indicating how similar the two words are in terms of their meanings.</li></ul><ul id="891b109f-5a89-464d-b20e-13ac85a4c07b" class="bulleted-list"><li style="list-style-type:disc"><strong>Distance Calculation</strong>:<mark class="highlight-red"> The smaller the cosine distance (1 - cosine similarity), the closer the words are in semantic space.</mark> WMD sums these distances to find the overall &quot;movement&quot; cost between the generated text and the reference text.</li></ul></details></li></ul><ul id="7b39cd1b-d562-4964-ae13-9f2c2e10fdc1" class="toggle"><li><details open=""><summary><mark class="highlight-brown">Work with FactScore:</mark></summary><p id="581369bd-abe1-45be-915a-135555e1e168" class="">when used default factscore code result was zero (factscore =0) as it was doing perfect matching for subject, object, and realtion.</p><p id="5deb4669-8519-4227-acde-3f4d9bc92707" class="">when I used lemma with word then matched the subject, object, and relation, it went from 0 to 0.01.</p></details></li></ul><ul id="1ec648cc-d81f-43c4-8bab-21d874f538ac" class="toggle"><li><details open=""><summary><mark class="highlight-brown">Work With PARENT Metric:</mark></summary><p id="7acc1357-97a8-44e5-b674-0ab9d95217ba" class="">This is an evaluation metric for the text generation model especially when the reference data is divergent.</p><p id="f4970204-6d19-445e-9f5d-19a2a92e96e6" class=""><strong>What is divergent?</strong>: In many datasets, the reference texts used for evaluation include information not present in the associated tables, leading to discrepancies.</p><p id="b02ad38d-1f88-4f50-9287-ce4746333d5a" class="">So when reference text is divergent, traditional metrics misrepresent the true quality of generated text which only considers the reference text for evaluation.</p><p id="6e170065-5a16-4970-b368-3515d472823e" class="">So the author has developed automatic metrics that better correlate with human judgment, especially in cases of divergence.</p><p id="d1e5aecb-f421-42bc-971c-46c390d306ce" class="">That metric is called PARENT (precision and recall entailed n-grams from the table).</p><p id="0b242430-e39b-4ded-96a3-4746dfb56517" class="">
</p><p id="9fbe5a7b-bd19-4848-af15-2861413c0380" class=""><mark class="highlight-brown"><strong>How precision and recall are calculated:</strong></mark></p><p id="b409570d-ea72-4a9d-b460-eb5b5b861df5" class="">1. <strong>Extracting Information from the Table, Reference, and Generated Text</strong></p><p id="700618be-bb1e-492c-9ded-69199b3afd1b" class=""><strong>Step 1.1: Extracting N-grams</strong></p><ul id="8843c086-361c-47ce-8137-0b6632fd7e51" class="bulleted-list"><li style="list-style-type:disc"><strong>N-grams</strong> are contiguous sequences of &#x27;n&#x27; items (words or tokens) from a given text. For example, for a sentence &quot;The Eiffel Tower is in Paris,&quot; the bigrams (2-grams) would be [&quot;The Eiffel&quot;, &quot;Eiffel Tower&quot;, &quot;Tower is&quot;, &quot;is in&quot;, &quot;in Paris&quot;].</li></ul><ul id="2d8828d7-78ae-490f-b8c2-14748df2a9bb" class="bulleted-list"><li style="list-style-type:disc"><strong>Extract N-grams</strong> from:<ul id="9af337f0-daf3-444a-9212-029896242254" class="bulleted-list"><li style="list-style-type:circle"><strong>Generated Text</strong>: The text produced by the model.</li></ul><ul id="d772f2e8-4461-46f6-bd8e-ce5de0214ea2" class="bulleted-list"><li style="list-style-type:circle"><strong>Reference Text</strong>: The human-written text that serves as a benchmark.</li></ul><ul id="2b2c5215-ef53-4141-a937-4a727efbba28" class="bulleted-list"><li style="list-style-type:circle"><strong>Table Data</strong>: The structured information provided as the source.</li></ul></li></ul><p id="d7dee512-95eb-4bdf-a9a1-8584d20b4b15" class=""><strong>Step 1.2: Entailment Model</strong></p><ul id="1b20f269-99d5-44a7-b733-b3e4ffc745d4" class="bulleted-list"><li style="list-style-type:disc"><strong>Entailment</strong>: The process of determining whether an n-gram in the generated text can be logically inferred from the table or the reference text.</li></ul><ul id="dfd7d658-9d04-4dde-ac8b-1e0af3accb5e" class="bulleted-list"><li style="list-style-type:disc"><strong>Simple Models</strong>: These can include:<ul id="fa5d58a3-84ff-4f5c-9766-e48843c2df34" class="bulleted-list"><li style="list-style-type:circle"><strong>Word Overlap Model</strong>: Determines entailment based on direct word matches.</li></ul><ul id="d64f5863-9c13-4b50-af92-a24431bfebdb" class="bulleted-list"><li style="list-style-type:circle"><strong>Co-occurrence Model</strong>: Uses statistical data to check if certain words frequently co-occur and, therefore, may infer logical connections.</li></ul></li></ul><p id="e1b93f43-5a49-4301-a075-539b686738aa" class="">2. <strong>Calculating Entailed Precision</strong></p><p id="9a7d4b74-6fdd-4a5b-b0cf-01ceb6b3e49b" class=""><strong>Step 2.1: Identify Correctly Entailed N-grams</strong></p><ul id="dfa99f13-14f8-4d2a-bede-5cafb3c9a43a" class="bulleted-list"><li style="list-style-type:disc"><strong>For each n-gram in the generated text</strong>:<ul id="45ac4914-e7b5-4d3f-865e-0c02145c28b4" class="bulleted-list"><li style="list-style-type:circle">Check if the n-gram can be <strong>inferred</strong> from the table. This means checking if the information in the n-gram is either directly present or can be logically deduced from the facts in the table.</li></ul><ul id="a8de5dd2-510f-4c35-a774-4c7dee0d596e" class="bulleted-list"><li style="list-style-type:circle">This involves matching the n-gram to either a corresponding n-gram in the table data or a logically related phrase.</li></ul></li></ul><p id="f9817b33-828a-4350-a8de-f218c6fd8301" class=""><strong>Step 2.2: Calculate Entailed Precision</strong></p><ul id="f8ad1b68-7397-4aae-a7d8-5c9c31d28b16" class="bulleted-list"><li style="list-style-type:disc"><strong>Formula</strong>:<br/>Entailed Precision=Total number of n-grams in generated textNumber of correctly entailed n-grams in generated text​<br/><p id="50beab7a-5b9b-4e32-b3eb-fb3384f7e9f0" class="">Entailed Precision=Number of correctly entailed n-grams in generated textTotal number of n-grams in generated text\text{Entailed Precision} = \frac{\text{Number of correctly entailed n-grams in generated text}}{\text{Total number of n-grams in generated text}}</p></li></ul><ul id="ce1e5e3f-6c8f-482a-b86b-05e314546b86" class="bulleted-list"><li style="list-style-type:disc"><strong>Explanation</strong>:<ul id="bb002590-d26c-41b6-b599-b3910da57600" class="bulleted-list"><li style="list-style-type:circle">This ratio represents how much of the generated text is accurate and supported by the table data.</li></ul><ul id="32208b2b-8732-409a-b254-c6f8b3aac27a" class="bulleted-list"><li style="list-style-type:circle">A higher precision means that a larger proportion of the generated text is factually correct.</li></ul></li></ul><p id="d7efc062-a6a0-47d3-8a54-f97da00048b3" class="">3. <strong>Calculating Entailed Recall</strong></p><p id="494e8248-136a-4e65-8b67-26869eb2650b" class=""><strong>Step 3.1: Identify N-grams Covered by the Generated Text</strong></p><ul id="176eb0fc-1e3d-463d-8ab2-8e73d2ae15e5" class="bulleted-list"><li style="list-style-type:disc"><strong>For each n-gram in the reference text</strong>:<ul id="ba6b33bf-9ee9-4604-b135-3a53cc1894ef" class="bulleted-list"><li style="list-style-type:circle">Check if the n-gram or a logically related n-gram is present in the generated text.</li></ul><ul id="9e00a24f-f636-4c93-99a3-d50c8a12696d" class="bulleted-list"><li style="list-style-type:circle">The check also involves considering whether the n-gram can be logically inferred from the table data (even if phrased differently).</li></ul></li></ul><p id="5d0651e1-a88f-4b10-bf98-aa056769789f" class=""><strong>Step 3.2: Calculate Entailed Recall</strong></p><ul id="bdb187bd-2a0b-4c22-822b-f23683233598" class="bulleted-list"><li style="list-style-type:disc"><strong>Formula</strong>:<br/>Entailed Recall=Total number of n-grams in reference textNumber of correctly entailed n-grams in generated text​<br/><p id="57152d61-ff8a-4961-ad63-72dc6818e2d0" class="">Entailed Recall=Number of correctly entailed n-grams in generated textTotal number of n-grams in reference text\text{Entailed Recall} = \frac{\text{Number of correctly entailed n-grams in generated text}}{\text{Total number of n-grams in reference text}}</p></li></ul><ul id="c3ca1873-81ec-436d-90f2-9996579f18fb" class="bulleted-list"><li style="list-style-type:disc"><strong>Explanation</strong>:<ul id="eb0e1ff2-a84c-473c-9378-5c1d883dd2f0" class="bulleted-list"><li style="list-style-type:circle">This ratio indicates how well the generated text covers the key information that should be included, as reflected in the reference text and the table.</li></ul><ul id="017b50d6-731c-4801-95ab-f4d993f67639" class="bulleted-list"><li style="list-style-type:circle">A higher recall means that the generated text includes more of the relevant information from the reference.</li></ul></li></ul><p id="d61866e7-ee16-4384-8ace-8611e013dce7" class="">4. <strong>Combining Entailed Precision and Recall</strong></p><p id="9e7ee3c7-528c-409a-b356-713f4e2a84b9" class=""><strong>Step 4.1: Calculate F1 Score</strong></p><ul id="3ab50a45-7378-4232-aefc-fd9bfc336f3f" class="bulleted-list"><li style="list-style-type:disc"><strong>F1 Score</strong> is often used to combine precision and recall into a single metric:<br/>F1 Score=2×Precision+RecallPrecision×Recall​<br/><p id="5ea4485f-7879-4117-b957-ba37ca38e758" class="">F1 Score=2×Precision×RecallPrecision+Recall\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}</p></li></ul><ul id="a4213754-6bc6-45c8-b2c7-5f1a655c764d" class="bulleted-list"><li style="list-style-type:disc"><strong>PARENT Score</strong>:<ul id="ec241581-a4a8-46ed-86dc-d94216b98cfc" class="bulleted-list"><li style="list-style-type:circle">The final PARENT score is essentially an F1 score that combines entailed precision and entailed recall, providing a balanced evaluation of both accuracy and completeness.</li></ul><p id="6fde923b-a322-4fc2-a044-f7866df203d6" class="">
</p></li></ul><p id="132cff1c-4710-431e-ae66-13f040d0e9a4" class=""><mark class="highlight-brown"><strong>What is co-occurance model?</strong></mark></p><p id="5e7c2c1a-9981-4df3-b462-53e271cb19cb" class=""><strong>Co-occurrence Model in Detail</strong></p><p id="d0117fd5-a074-4f95-ab1e-a507e699be16" class="">1. <strong>Understanding Co-occurrence</strong></p><ul id="334bcf60-1389-4182-885c-5a1b82832fcd" class="bulleted-list"><li style="list-style-type:disc"><strong>Co-occurrence</strong> refers to how frequently two or more words appear together within a certain context, such as in the same sentence, paragraph, or document.</li></ul><ul id="2759a629-ad33-4d68-9158-3076828a24a4" class="bulleted-list"><li style="list-style-type:disc">The idea is that words that frequently appear together (co-occur) have a certain relationship or association, which can be leveraged to infer whether one piece of information supports another.</li></ul><p id="ce6382eb-c4c1-4ca9-9385-c94cc310f022" class="">2. <strong>Building the Co-occurrence Model</strong></p><ul id="a358fa1a-8a41-4841-beb1-d0ec782ea61a" class="bulleted-list"><li style="list-style-type:disc"><strong>Step 1: Collect Co-occurrence Statistics</strong>:<ul id="8bd35261-89f7-47b3-8250-72d64442e671" class="bulleted-list"><li style="list-style-type:circle">Before applying the co-occurrence model to evaluate generated text, you need a large corpus of text from which you can gather co-occurrence statistics.</li></ul><ul id="e637f887-3d8a-4820-9af2-97d1383a7f6e" class="bulleted-list"><li style="list-style-type:circle">This corpus could be similar in domain to your task (e.g., Wikipedia articles if you are generating text about entities like places or people).</li></ul><ul id="ff87ad87-ac79-4bd3-b8a0-a4dc6e81b549" class="bulleted-list"><li style="list-style-type:circle">The statistics track how often pairs of words (or n-grams) appear together in the corpus.</li></ul></li></ul><ul id="3cb2de3a-652c-4ee6-98dd-356ae8ffcba6" class="bulleted-list"><li style="list-style-type:disc"><strong>Step 2: Create a Co-occurrence Matrix</strong>:<ul id="1d46ec33-f16d-425b-8828-44775565b693" class="bulleted-list"><li style="list-style-type:circle">A <strong>co-occurrence matrix</strong> is constructed, where each cell <code>(i, j)</code> represents the frequency with which word <code>i</code> and word <code>j</code> appear together within a defined window (e.g., within the same sentence or within a few words of each other).</li></ul><ul id="f844398c-1b41-493a-bd68-aa88c2f768a1" class="bulleted-list"><li style="list-style-type:circle">The matrix could be symmetric (if the order of words doesn&#x27;t matter) or asymmetric (if the order matters, such as for n-grams).</li></ul></li></ul><p id="64841496-b2ec-495b-8f44-0c5a0192b60b" class="">3. <strong>Using the Co-occurrence Model to Evaluate Entailment</strong></p><ul id="d75943a7-d697-4781-890e-db76b24341be" class="bulleted-list"><li style="list-style-type:disc"><strong>Step 1: Match Generated Text with Table Data</strong>:<ul id="4bdcad81-4a0d-45cc-afdf-e7595498ac2b" class="bulleted-list"><li style="list-style-type:circle">For each n-gram in the generated text, check if the words or phrases in that n-gram have a high co-occurrence probability with relevant words or phrases from the table data.</li></ul><ul id="8fd116f8-1525-40aa-ac45-093c41feb51e" class="bulleted-list"><li style="list-style-type:circle">The idea is that if the generated text&#x27;s n-grams frequently co-occur with the n-grams from the table data in the corpus, they are likely to be logically connected.</li></ul></li></ul><ul id="65cc52a9-eeb9-42e6-ab5b-7312b3f7f640" class="bulleted-list"><li style="list-style-type:disc"><strong>Step 2: Calculate Entailment Probability</strong>:<ul id="033fc7ad-c4da-4b97-b600-909c7811e1aa" class="bulleted-list"><li style="list-style-type:circle">The entailment probability for an n-gram is derived from the co-occurrence scores between the generated n-gram and the table data.</li></ul><ul id="ad736b3f-dba5-4536-bbcb-abe582547e97" class="bulleted-list"><li style="list-style-type:circle"><strong>High Co-occurrence Probability</strong>: Indicates that the n-gram in the generated text is likely to be entailed by the table because similar expressions frequently appear together in the corpus.</li></ul><ul id="9ef2163b-4850-45d3-83b0-f93fe4058723" class="bulleted-list"><li style="list-style-type:circle"><strong>Low Co-occurrence Probability</strong>: Suggests that the n-gram is less likely to be supported by the table data.</li></ul></li></ul><ul id="0763e5a5-8c22-47cd-a563-bc918c7408be" class="bulleted-list"><li style="list-style-type:disc"><strong>Step 3: Aggregate Entailment Scores</strong>:<ul id="4beda443-ae3b-41a4-b9da-9e4f889a1f40" class="bulleted-list"><li style="list-style-type:circle">Aggregate the entailment probabilities across all n-grams in the generated text to determine the overall precision and recall.</li></ul><p id="ddc27c8f-8bf1-4b48-ae0c-caddc34d3e82" class="">
</p></li></ul><p id="c131e3ed-a17a-45ab-836e-a1cb52909327" class="">
</p><p id="09513f4b-478b-4426-a1b7-dd0e23d8bb09" class="">
</p><p id="62538f43-966d-40f3-8331-8d709eae71be" class="">
</p></details></li></ul><ul id="b588df68-e82b-4827-82b6-a2e63d70927a" class="toggle"><li><details open=""><summary></summary></details></li></ul><p id="d7ec9489-929a-4f06-bd8c-859b0a84c3b4" class="">
</p><p id="fb3bf407-5928-404f-bce3-a745c59d0ac2" class=""><br/><br/></p><p id="c37115d2-f555-4654-aaff-09160296540e" class="">
</p><p id="79455154-fc67-457e-aa3e-a7b707c0d7ae" class="">
</p><p id="a262ccb8-62da-40b2-9c10-0929573c72de" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>